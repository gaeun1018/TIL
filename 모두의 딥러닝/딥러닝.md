# 딥러닝

![image](https://thebook.io/img/080324/018.jpg)

딥러닝은 인간의 두뇌에서 영감을 얻은 방식으로 데이터를 처리하도록 컴퓨터를 가르치는 인공 지능 방식이다.


## 딥러닝의 계산 원리
- 선형 회귀(linear regression)
- 로지스틱 회귀

<br>

### 선형 회귀

*선형 회귀란 독립 변수 x를 사용해 종속 변수 y의 움직임을 예측하고 설명하는 작업이다.*   

독립적으로 변할 수 있는 값 `x`를 `독립 변수`라고 한다. 또한, 이 독립 변수에 따라 종속적으로 변하는 `y`를 `종속 변수`라고 한다.    
하나의 x 값만으로도 y 값을 설명할 수 있다면 `단순 선형 회귀(simple linear regression)`라고 한다. 또한, x 값이 여러 개 필요하다면 `다중 선형 회귀(multiple linear regression)`라고 한다.

선형회귀는 점들의 특징을 가장 잘 나타내는 선을 그리는 과정이다.   
<br>

    y = ax+b


위의 식에서 *x*는 독립 변수이고 *y*는 종속 변수이다.  
정확하게 계산하려면 기울기 *a*와 *y* 절편 *b*의 값을 예측해야한다.

<br>

**최소 제곱법**

![image](https://thebook.io/img/080324/fx-44.jpg)
![image](https://thebook.io/img/080324/fx-48.jpg)

최소 제곱법을 이용하면 *a*와 *b*의 값을 구할 수 있다.

<br>

**코드로 구현하기**
```python
x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])

# 평균값 계산
mx = np.mean(x)
my = np.mean(y)

# 분모 계산
divisor = ➊sum([(i - mx)➋**2 ➌for i in x])

# 분자 계산
def top(x, mx, y, my):
    d = 0
    for i in range(len(x)):
        d += (x[i] - mx) * (y[i] - my)
    return d

dividend = top(x, mx, y, my)

a = dividend / divisor
b = my - (mx * a)
```
<br>

**평균 제곱 오차(Mean Square Error, MSE)**

오차(error)를 제곱한 값의 평균으로 이 값이 작을수록 훌륭한 알고리즘이다.

![image](https://thebook.io/img/080324/fx-3.jpg)

<br>

**코드로 구현하기**
```python
# 임의로 a와 b를 설정
fake_a = 3
fake_b = 76

def predict(x):
    return fake_a * x + fake_b

predict_result = []

# x의 값을 넣어 예측값을 빈 리스트에 채우는 함수
for i in range(len(x)):
    predict_result.append(predict(x[i]))
    print(x[i], y[i], predict(x[i]))

# 평균 제곱 오차 계산 함수
n = len(x)  
def mse(y, y_pred):
    return (1/n) * sum((y - y_pred)**2)
```

<br>

**선형 회귀**란 임의의 직선을 그어 이에 대한 평균 제곱 오차를 구하고, 이 값을 가장 작게 만들어 주는 a 값과 b 값을 찾아가는 작업이다.