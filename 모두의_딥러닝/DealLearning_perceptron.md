# 딥러닝

## 신경망

![image](https://thebook.io/img/080324/112_1.jpg)

퍼셉트론은 입력 값을 여러 개 받아 출력을 만드는데, 이때 입력 값에 가중치를 조절할 수 있게 만들어 최초로 ‘학습’을 하게 했다.  
  
시그모이드를 활성화 함수로 사용한 것이 바로 앞서 배웠던 로지스틱 회귀이다.  
  
  <br>
퍼셉트론의 한계를 설명할 때 XOR 문제가 사용된다.  

<br>

![image](https://thebook.io/img/080324/114.jpg)

XOR의 경우 선을 그어 구분할 수가 없다.

<br>

## 다층 퍼셉트론
**퍼셉트론 두 개를 한 번에 계산**하기 위해 퍼셉트론 두 개를 각각 처리하는 은닉층(hidden layer)을 만든다.

![image](https://thebook.io/img/080324/117_1.jpg)

NAND와 OR을 처리 후 두 연산의 결과를 AND 처리하면 원하는 결과를 얻을 수 있다.

<br>

![image](https://thebook.io/img/080324/118.jpg)

x1과 x2는 입력 값인데, 각 값에 가중치(w)를 곱하고 바이어스(b)를 더해 은닉층으로 전송한다. 이 값들이 모이는 은닉층의 중간 정거장을 노드(node)라고 하며, 은닉층에 취합된 값들은 활성화 함수를 통해 다음으로 보낸다.

<br>

**코드로 구현하기**

[Colab_XOR][colablink]

[colablink]: colab\XOR.ipynb  

<br>

## 오차 역전파

![image](https://thebook.io/img/080324/126_2.jpg)

은닉층이 생기며 경사하강법을 여러번 실행해야 한다.

![image](https://thebook.io/img/080324/127.jpg)


첫 번째 가중치 업데이트 공식  $$(yo1- y실제 값) . yo1(1-yo1) . yh1$$  


두 번째 가중치 업데이트 공식 $$(𝛅yo1 . w31+ 𝛅yo2 . w41)yh1(1-yh1) . x1$$

두 식 모두 ‘out(1-out)’ 형태(델타식)를 취하고 있기 때문에 은닉층이 여러개 생기더라도 계산을 할 수 있게 된다.


![image](https://thebook.io/img/080324/129_1.jpg)

그러나 깊은 층을 만들어 보니 출력층에서 시작된 가중치 업데이트가 처음 층까지 전달되지 않는 현상이 생기는 문제가 발견되었다.  
활성화 함수로 사용된 시그모이드 함수의 특성 때문인데, 시그모이드 함수의 미분값의 최대가 0.25이기 때문에 계속 곱하다보면 0에 가까워졌기 때문이다.

<br>

![image](https://thebook.io/img/080324/129_2.jpg)

이를 해결하기 위해 렐루 함수가 등장하였다.  
$x$가 0보다 작을 때 모든 값을 0으로 처리하고, 0보다 큰 값은 $x$를 그대로 사용하는 방법이다. 따라서 활성화 함수로 렐루를 쓰면 여러 번 오차 역전파가 진행되어도 맨 처음 층까지 값이 남아 있게 된다.


![image](https://thebook.io/img/080324/131_2.jpg)

속도와 정확도 문제를 해결하기 위한 확률적 경사하강법과 모멘텀의 개념도 등장하였다.  
확률적 경사하강법은 전체 데이터를 사용하는 것이 아니라, 랜덤하게 추출한 일부 데이터만 사용하기 때문에 빠르고 더 자주 업데이트할 수 있다는 장점이 있다.  
모멘텀은 양수(+) 방향으로 한 번, 음수(-) 방향으로 한 번 수정을 하기 때문에 지그재그로 일어나는 현상이 줄어들고, 이전 이동 값을 고려해 일정 비율만큼 다음 값을 결정하므로 관성 효과를 낼 수 있다.